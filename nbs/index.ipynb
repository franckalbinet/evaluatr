{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluatr\n",
    "\n",
    "> AI-Powered Evaluation Report Analysis and Framework Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![PyPI](https://img.shields.io/pypi/v/evaluatr)](https://pypi.org/project/evaluatr/)\n",
    "[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n",
    "[![Documentation](https://img.shields.io/badge/docs-GitHub%20Pages-blue)](https://franckalbinet.github.io/evaluatr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Evaluatr?\n",
    "\n",
    "`Evaluatr` is an AI-powered system designed to automate the complex task of mapping evaluation reports against structured frameworks. Initially developed for [IOM (International Organization for Migration)](https://www.iom.int) evaluation reports and the [Strategic Results Framework (SRF)](https://srf.iom.int), it transforms a traditionally manual, time-intensive process into an intelligent, interpretable workflow.\n",
    "\n",
    "The system maps evaluation reports‚Äîoften 150+ pages of heterogeneous content‚Äîagainst hierarchical frameworks like the SRF, which contains objectives, enablers, and cross-cutting priorities, each with specific outcomes, outputs, and indicators. Evaluatr targets the output level for optimal granularity and connects to broader frameworks like the [Sustainable Development Goals (SDGs)](https://sdgs.un.org) for interoperability.\n",
    "\n",
    "Beyond automation, Evaluatr prioritizes **interpretability and human-AI collaboration**. IOM evaluators can understand the mapping process, audit AI decisions, perform error analysis, build training datasets over time, and create robust evaluation pipelines‚Äîensuring the AI system aligns with business needs through actionable, transparent, auditable methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge We Solve\n",
    "\n",
    "IOM evaluators possess deep expertise in mapping evaluation reports against frameworks like the Strategic Results Framework (SRF), but face significant operational challenges when processing reports that often exceed 150 pages of diverse content across multiple projects and contexts.\n",
    "\n",
    "The core challenges are:\n",
    "- **Time-intensive process**: Hundreds of staff-hours required per comprehensive mapping exercise\n",
    "- **Individual consistency**: Even expert evaluators may categorize the same content differently across sessions\n",
    "- **Cross-evaluator consistency**: Different evaluators may interpret and map identical content to different framework outputs\n",
    "- **Scale vs. thoroughness**: Growing volume of evaluation reports creates pressure to choose between speed and comprehensive analysis\n",
    "\n",
    "IOM needs a solution that leverages evaluators' expertise while addressing these operational bottlenecks‚Äîaccelerating the mapping process while maintaining the consistency and thoroughness that manual review currently struggles to achieve at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features\n",
    "\n",
    "### 1. Document Preparation Pipeline ‚úÖ **Available**\n",
    "- **Repository Processing**: Read and preprocess IOM evaluation report repositories with standardized outputs\n",
    "- **Automated Downloads**: Batch download of evaluation documents from diverse sources\n",
    "- **OCR Processing**: Convert scanned PDFs to searchable text using Optical Character Recognition (OCR) technology\n",
    "- **Content Enrichment**: Fix OCR-corrupted headings and enrich documents with AI-generated image descriptions for high-quality input data\n",
    "\n",
    "### 2. Intelligent Mapping üöß **In Development**\n",
    "- **Agentic Framework Mapping**: Use DSPy-powered agents for traceable, interpretable mapping of reports against evaluation frameworks like the IOM Strategic Results Framework (SRF)\n",
    "- **Command-line Interface**: Streamlined pipeline execution through easy-to-use CLI tools\n",
    "\n",
    "### 3. Knowledge Synthesis üìã **Planned**\n",
    "- **Knowledge Cards**: Generate structured summaries for downstream AI tasks like proposal writing and synthesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ô∏è Installation & Setup\n",
    "\n",
    "### From PyPI (Recommended)\n",
    "```bash\n",
    "pip install evaluatr\n",
    "```\n",
    "\n",
    "### From GitHub\n",
    "```bash\n",
    "pip install git+https://github.com/franckalbinet/evaluatr.git\n",
    "```\n",
    "\n",
    "### Development Installation\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/franckalbinet/evaluatr.git\n",
    "cd evaluatr\n",
    "\n",
    "# Install in development mode\n",
    "pip install -e .\n",
    "\n",
    "# Make changes in nbs/ directory, then compile:\n",
    "nbdev_prepare\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "This project uses [nbdev](https://nbdev.fast.ai) for literate programming - see the Development section for more details.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "Create a `.env` file in your project root with your API keys:\n",
    "\n",
    "```bash\n",
    "MISTRAL_API_KEY=\"your_mistral_api_key\"\n",
    "GEMINI_API_KEY=\"your_gemini_api_key\"\n",
    "```\n",
    "\n",
    "**Note**: Evaluatr uses `llmlite` and `dspy` for LLM interactions, giving you flexibility to use any compatible language model provider beyond the examples above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading an IOM Evaluation Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1a57974ab89d7280988aa6b706147ce1\n",
      "Title: EX-POST EVALUATION OF THE PROJECT:  NIGERIA: STRENGTHENING REINTEGRATION FOR RETURNEES (SRARP)  - PHASE II\n",
      "Documents: 2\n",
      "---\n",
      "ID: c660e774d14854e20dc74457712b50ec\n",
      "Title: FINAL EVALUATION OF THE PROJECT: STRENGTHEN BORDER MANAGEMENT AND SECURITY IN MALI AND NIGER THROUGH CAPACITY BUILDING OF BORDER AUTHORITIES AND ENHANCED DIALOGUE WITH BORDER COMMUNITIES\n",
      "Documents: 2\n",
      "---\n",
      "ID: 2cae361c6779b561af07200e3d4e4051\n",
      "Title: Final Evaluation of the project \"SUPPORTING THE IMPLEMENTATION OF AN E RESIDENCE PLATFORM IN CABO VERDE\"\n",
      "Documents: 2\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from evaluatr.readers import IOMRepoReader\n",
    "\n",
    "# Initialize reader with your Excel file\n",
    "reader = IOMRepoReader('files/test/eval_repo_iom.xlsx')\n",
    "\n",
    "# Process the repository\n",
    "evaluations = reader()\n",
    "\n",
    "# Each evaluation is a standardized dictionary\n",
    "for eval in evaluations[:3]:  # Show first 3\n",
    "    print(f\"ID: {eval['id']}\")\n",
    "    print(f\"Title: {eval['meta']['Title']}\")\n",
    "    print(f\"Documents: {len(eval['docs'])}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting it to JSON:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "reader.to_json('processed_evaluations.json')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading evaluation documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#24) ['Downloaded Internal%20Evaluation_NG20P0516_MAY_2023_FINAL_Abderrahim%20EL%20MOULAT.pdf','Downloaded RR0163_Evaluation%20Brief_MAY_%202023_Abderrahim%20EL%20MOULAT.pdf','Downloaded IB0238_Evaluation%20Brief_FEB_%202023_Abderrahim%20EL%20MOULAT.pdf','Downloaded Internal%20Evaluation_IB0238__FEB_2023_FINAL%20RE_Abderrahim%20EL%20MOULAT.pdf','Downloaded IB0053_Evaluation%20Brief_SEP_%202022_Abderrahim%20EL%20MOULAT.pdf','Downloaded Internal%20Evaluation_IB0053_OCT_2022_FINAL_Abderrahim%20EL%20MOULAT_0.pdf','Downloaded Internal%20Evaluation_NC0030_JUNE_2022_FINAL_Abderrahim%20EL%20MOULAT_0.pdf','Downloaded NC0030_Evaluation%20Brief_June%202022_Abderrahim%20EL%20MOULAT.pdf','Downloaded CD0015_Evaluation%20Brief_May%202022_Abderrahim%20EL%20MOULAT.pdf','Downloaded Projet%20CD0015_Final%20Evaluation%20Report_May_202_Abderrahim%20EL%20MOULAT.pdf','Downloaded Internal%20Evaluation_Retour%20Vert_JUL_2021_Fina_Abderrahim%20EL%20MOULAT.pdf','Downloaded NC0012_Evaluation%20Brief_JUL%202021_Abderrahim%20EL%20MOULAT.pdf','Downloaded Nigeria%20GIZ%20Internal%20Evaluation_JANUARY_2021__Abderrahim%20EL%20MOULAT.pdf','Downloaded Nigeria%20GIZ%20Project_Evaluation%20Brief_JAN%202021_Abderrahim%20EL%20MOULAT_0.pdf','Downloaded Evaluation%20Brief_ARCO_Shiraz%20JERBI.pdF','Downloaded Final%20evaluation%20report_ARCO_Shiraz%20JERBI_1.pdf','Downloaded Management%20Response%20Matrix_ARCO_Shiraz%20JERBI.pdf','Downloaded IOM%20MANAGEMENT%20RESPONSE%20MATRIX.pdf','Downloaded IOM%20Niger%20-%20MIRAA%20III%20-%20Final%20Evaluation%20Report%20%28003%29.pdf','Downloaded CE.0369%20-%20IDEE%20-%20ANNEXE%201%20-%20Rapport%20Recherche_Joanie%20DUROCHER_0.pdf'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluatr.downloaders import download_docs\n",
    "from pathlib import Path\n",
    "\n",
    "fname = 'files/test/evaluations.json'\n",
    "base_dir = Path(\"files/test/pdf_library\")\n",
    "download_docs(fname, base_dir=base_dir, n_workers=0, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Processing\n",
    "\n",
    "Convert PDF evaluation reports into structured markdown files with extracted images:\n",
    "\n",
    "```python\n",
    "from evaluatr.ocr import process_single_evaluation_batch\n",
    "from pathlib import Path\n",
    "\n",
    "# Process a single evaluation report\n",
    "report_path = Path(\"path/to/your/evaluation_report_folder\")\n",
    "output_dir = Path(\"md_library\")\n",
    "\n",
    "process_single_evaluation_batch(report_path, output_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Structure:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "md_library/\n",
    "‚îú‚îÄ‚îÄ evaluation_id/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ page_1.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ page_2.md\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ img/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ img-0.jpeg\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ img-1.jpeg\n",
    "```\n",
    "\n",
    "**Example markdown page with image reference as generated by Mistral OCR:**\n",
    "\n",
    "```markdown\n",
    "The evaluation followed the Organisation of Economic Cooperation and Development/Development Assistance Committee (OECD/DAC) evaluation criteria and quality standards. The evaluation ...\n",
    "\n",
    "FIGURE 2. OECD/DAC CRITERIA FOR EVALUATIONS\n",
    "![img-2.jpeg](img-2.jpeg)\n",
    "\n",
    "Each evaluation question includes the main data collection ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch OCR Processing\n",
    "\n",
    "Process multiple evaluation reports efficiently using Mistral's batch OCR API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from evaluatr.ocr import process_all_reports_batch\n",
    "from pathlib import Path\n",
    "\n",
    "# Get all evaluation report directories\n",
    "reports_dir = Path(\"path/to/all/evaluation_reports\")\n",
    "report_folders = [d for d in reports_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "# Process all reports using batch OCR for efficiency\n",
    "process_all_reports_batch(report_folders, md_library_path=\"md_library\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of batch processing:**\n",
    "- Significantly faster than processing PDFs individually\n",
    "- Cost-effective through Mistral's batch API pricing (expect $0.5 per 1,000 pages)\n",
    "- Automatic job monitoring and result retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Enrichment\n",
    "\n",
    "While Mistral OCR excels at text extraction, it often struggles with heading hierarchy detection, producing inconsistent markdown levels that break document structure. Clean, properly nested headings are crucial for agentic AI systems to retrieve content hierarchically‚Äîmimicking how experienced evaluation analysts navigate reports by section and subsection (as you'll see in the upcoming `mappr` module). Additionally, evaluation reports contain rich visual evidence through charts, graphs, and diagrams that standard OCR simply references as image links. The enrichr module addresses these \"garbage in, garbage out\" challenges by fixing structural issues and converting visual content into searchable, AI-readable descriptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from evaluatr.enrichr import fix_doc_hdgs, enrich_images\n",
    "from pathlib import Path\n",
    "\n",
    "# Fix heading hierarchy in OCR'd document\n",
    "doc_path = Path(\"md_library/evaluation_id\")\n",
    "fix_doc_hdgs(doc_path)\n",
    "\n",
    "# Enrich images with descriptive text\n",
    "pages_dir = doc_path / \"enhanced\"\n",
    "img_dir = doc_path / \"img\"\n",
    "enrich_images(pages_dir, img_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "- **Full Documentation**: [GitHub Pages](https://fr.anckalbi.net/evalstack/)\n",
    "- **API Reference**: Available in the documentation\n",
    "- **Examples**: See the `nbs/` directory for Jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing\n",
    "\n",
    "### Development Philosophy\n",
    "\n",
    "Evaluatr is built using [**nbdev**](https://nbdev.fast.ai), a literate programming framework that allows us to develop code, documentation, and tests together in Jupyter notebooks. This approach offers several advantages:\n",
    "\n",
    "- **Documentation-driven development**: Code and explanations live side-by-side, ensuring documentation stays current\n",
    "- **Reproducible research**: Each module's development process is fully transparent and reproducible\n",
    "- **Collaborative friendly**: Notebooks make it easier for domain experts to understand and contribute to the codebase\n",
    "\n",
    "**fastcore** provides the foundational utilities that power this approach, offering enhanced Python functionality and seamless integration between notebooks and production code.\n",
    "\n",
    "### Development Setup\n",
    "\n",
    "We welcome contributions! Here's how you can help:\n",
    "\n",
    "We welcome contributions! Here's how you can help:\n",
    "\n",
    "1. **Fork** the repository\n",
    "2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)\n",
    "3. **Make** your changes in the `nbs/` directory\n",
    "4. **Compile** with `nbdev_prepare`\n",
    "5. **Commit** your changes (`git commit -m 'Add amazing feature'`)\n",
    "6. **Push** to the branch (`git push origin feature/amazing-feature`)\n",
    "7. **Open** a Pull Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Setup\n",
    "\n",
    "```bash\n",
    "# Install development dependencies\n",
    "pip install -e .\n",
    "\n",
    "# Make changes in nbs/ directory\n",
    "# ...\n",
    "\n",
    "# Compile changes to evalstack package\n",
    "nbdev_prepare\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "`Evaluatr` is built on these key Python packages:\n",
    "- **fastcore** & **pandas** - Core data processing and utilities\n",
    "- **mistralai** & **litellm** - AI/LLM integration for OCR and enrichment\n",
    "- **dspy** & **toolslm** - Structured AI programming and tool integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support\n",
    "\n",
    "- **Issues**: [GitHub Issues](https://github.com/franckalbinet/evalstack/issues)\n",
    "- **Discussions**: [GitHub Discussions](https://github.com/franckalbinet/evalstack/discussions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
