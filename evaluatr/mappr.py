"""Scale up evaluation report mapping against evaluation frameworks using agentic workflows"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/07_mappr.ipynb.

# %% auto 0
__all__ = ['GEMINI_API_KEY', 'cfg', 'traces_dir', 'lm', 'select_section_sp', 'summarize_sp', 'tr_ctx', 'find_section_path',
           'get_content_tool', 'flatten_sections', 'extract_content', 'format_enabler_theme',
           'format_crosscutting_theme', 'format_gcm_theme', 'format_srf_output', 'SelectSectionOutput',
           'SummarizeContentOutput', 'EvaluateEvidenceOutput', 'State', 'parse_response',
           'format_sections_for_selection', 'select_section', 'summarize_content', 'evaluate_evidence', 'limit',
           'Stage', 'TraceContext', 'setup_logger', 'setup_trace_logging', 'log_analysis_event', 'SectionSelection',
           'Assessment', 'Synthesis', 'ThemeAnalyzer', 'PipelineResults', 'PipelineOrchestrator',
           'get_stage1_covered_context', 'get_filtered_srf_output_ids', 'get_combined_context']

# %% ../nbs/07_mappr.ipynb 5
from pathlib import Path
from functools import reduce
from toolslm.md_hier import *
from rich import print
import json
from fastcore.all import *
from enum import Enum
import logging
import uuid
from datetime import datetime
from typing import List, Callable
import dspy
from asyncio import Semaphore, gather, sleep
import time
from collections import defaultdict
import copy

from pydantic import BaseModel, Field
from typing import List

from .frameworks import (EvalData, 
                                 IOMEvalData, 
                                 FrameworkInfo, 
                                 Framework,
                                 FrameworkCat,
                                 find_srf_output_by_id)

#from evaluatr.db_traces import TraceDB, Trace
from fastlite import database

from lisette import Chat, AsyncChat
import json

# %% ../nbs/07_mappr.ipynb 6
from dotenv import load_dotenv
import os

load_dotenv()
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

# %% ../nbs/07_mappr.ipynb 7
cfg = AttrDict({
    'lm': 'gemini/gemini-2.0-flash',
    'api_key': GEMINI_API_KEY,
    'max_tokens': 8192,
    'track_usage': False,
    'call_delay': 0.1, # in seconds
    'semaphore': 30,
    'dirs': AttrDict({
        'data': '.evaluatr',
        'trace': 'traces'
    }),
    'verbosity': 1,
    'cache': AttrDict({
        'is_active': False,
        'delay': 0.05 # threshold in seconds below which we consider the response is cached
    }),
    'max_iter': 10
})

# %% ../nbs/07_mappr.ipynb 8
traces_dir = Path.home() / cfg.dirs.data / cfg.dirs.trace
traces_dir.mkdir(parents=True, exist_ok=True)

# %% ../nbs/07_mappr.ipynb 12
lm = dspy.LM(cfg.lm, api_key=cfg.api_key, cache=cfg.cache.is_active)
dspy.configure(lm=lm)

# %% ../nbs/07_mappr.ipynb 17
def find_section_path(
    hdgs: dict, # The nested dictionary structure
    target_section: str # The section name to find
) -> list: # The nested key path for the given section name
    "Find the nested key path for a given section name."
    def search_recursive(current_dict, path=[]):
        for key, value in current_dict.items():
            current_path = path + [key]
            if key == target_section:
                return current_path
            if isinstance(value, dict):
                result = search_recursive(value, current_path)
                if result:
                    return result
        return None
    
    return search_recursive(hdgs)

# %% ../nbs/07_mappr.ipynb 21
def get_content_tool(
    hdgs: dict, # The nested dictionary structure
    keys_list: list, # The list of keys to navigate through
    ) -> str: # The content of the section
    "Navigate through nested levels using the exact key strings."
    return reduce(lambda current, key: current[key], keys_list, hdgs).text

# %% ../nbs/07_mappr.ipynb 23
def flatten_sections(hdgs, path=[]):
    """Extract flat list of (key, full_path) tuples from nested hdgs"""
    sections = []
    for key, value in hdgs.items():
        current_path = path + [key]
        sections.append((key, current_path))
        if isinstance(value, dict):
            sections.extend(flatten_sections(value, current_path))
    return sections

# %% ../nbs/07_mappr.ipynb 24
def extract_content(section_key: str, sections_lookup: dict, hdgs: dict) -> str:
    path = sections_lookup[section_key]
    return get_content_tool(hdgs, path)

# %% ../nbs/07_mappr.ipynb 27
def format_enabler_theme(
    theme: EvalData # The theme object
    ) -> str: # The formatted theme string
    "Format SRF enabler into structured text for LM processing."
    parts = [
        f'## Enabler {theme.id}: {theme.title}',
        '### Description', 
        theme.description
    ]
    return '\n'.join(parts)

# %% ../nbs/07_mappr.ipynb 30
def format_crosscutting_theme(
    theme: EvalData # The theme object
    ) -> str: # The formatted theme string
    "Format SRF cross-cutting into structured text for LM processing."
    parts = [
        f'## Cross-cutting {theme.id}: {theme.title}',
        '### Description', 
        theme.description
    ]
    return '\n'.join(parts)

# %% ../nbs/07_mappr.ipynb 33
def format_gcm_theme(
    theme: dict # The GCM theme object from gcm_small
    ) -> str: # The formatted theme string
    "Format GCM objective into structured text for LM processing."
    parts = [
        f'## GCM Objective {theme["id"]}: {theme["title"]}',
        '### Core Theme', 
        theme["core_theme"]
    ]
    
    if theme.get("key_principles"):
        parts.extend(['### Key Principles', ', '.join(theme["key_principles"])])
    
    if theme.get("target_groups"):
        parts.extend(['### Target Groups', ', '.join(theme["target_groups"])])
        
    if theme.get("main_activities"):
        parts.extend(['### Main Activities', ', '.join(theme["main_activities"])])
    
    return '\n'.join(parts)

# %% ../nbs/07_mappr.ipynb 36
def format_srf_output(output_context: dict) -> str:
    "Format SRF output with full hierarchical context for LM processing."
    parts = [
        f'## SRF Output {output_context["output"]["id"]}: {output_context["output"]["title"]}',
        '### Strategic Context',
        f'**Objective {output_context["objective"]["id"]}**: {output_context["objective"]["title"]}',
        f'**Long    -term Outcome {output_context["long_outcome"]["id"]}**: {output_context["long_outcome"]["title"]}',
        f'**Short-term Outcome {output_context["short_outcome"]["id"]}**: {output_context["short_outcome"]["title"]}'
    ]
    
    return '\n'.join(parts)

# %% ../nbs/07_mappr.ipynb 40
class SelectSectionOutput(BaseModel):
    "Select the next most relevant section based on current evidence summary and gaps"
    section_index: int
    reasoning: str

# %% ../nbs/07_mappr.ipynb 42
class SelectSectionOutput(BaseModel):
    "Select the next most relevant section based on current evidence summary and gaps"
    section_index: int
    reasoning: str

# %% ../nbs/07_mappr.ipynb 43
class SummarizeContentOutput(BaseModel):
    "Summarize the content of a section and identify the key findings"
    summary: str
    key_findings: List[str]

# %% ../nbs/07_mappr.ipynb 44
class EvaluateEvidenceOutput(BaseModel):
    theme_covered: bool
    coverage_reasoning: str
    gaps_identified: str
    should_continue: bool

# %% ../nbs/07_mappr.ipynb 45
class State(BaseModel):
    theme: str
    prior_coverage_context: str = ""
    section_summaries: List[dict] = []  # Renamed from evidences
    explored_sections: List[str] = []
    available_sections: List[str]
    evaluation_history: List[dict] = []  # Track reasoning evolution
    iterations_completed: int = 0
    theme_covered: bool = False
    coverage_reasoning: str = ""
    gaps_identified: str = ""
    stop_reason: str = ""

# %% ../nbs/07_mappr.ipynb 47
select_section_sp = """You are analyzing an evaluation report to determine if it covers a specific theme.

Given:
- The theme being analyzed
- Current evaluation state (summaries collected, evaluation reasoning so far)
- Available sections (as JSON array of [index, section_name] pairs)

Your task: Select the next section most likely to contain relevant evidence, considering:
- What gaps were identified in previous evaluations
- Which sections haven't been explored yet
- Where you're most likely to find missing evidence

Output JSON with:
- section_index: the index number from the pair (integer)
- reasoning: why you chose this section based on current gaps and needs
"""

# %% ../nbs/07_mappr.ipynb 48
summarize_sp = """You are summarizing content from an evaluation report section.

Your task: Extract and condense the key points relevant to the theme being analyzed.

This summary will be used to:
- Maintain context across iterations without inflating the prompt
- Provide the evaluation step with essential information from this section

Keep it concise but capture:
- Main findings or claims related to the theme
- Supporting evidence (data, quotes, examples)
- Methodological details if relevant

Output JSON with:
- summary: concise summary of the content
- key_findings: list of specific findings relevant to the theme
"""

# %% ../nbs/07_mappr.ipynb 50
def parse_response(result):
    "Extract JSON from Lisette response"
    return json.loads(result.choices[0].message.content)

# %% ../nbs/07_mappr.ipynb 51
def format_sections_for_selection(available_sections: List[str]) -> str:
    "Format available sections as indexed JSON array"
    return json.dumps([
        [i+1, s] for i, s in enumerate(available_sections)
    ], indent=2)

# %% ../nbs/07_mappr.ipynb 52
async def select_section(
    state: State,
    model: str = 'gemini/gemini-2.0-flash'
) -> dict:
    "Select next section to explore based on current state"
    chat = AsyncChat(model=model, sp=select_section_sp, temp=0)
    
    sections_json = format_sections_for_selection(state.available_sections)
    
    # Format evaluation history for context
    eval_summary = "\n".join([
        f"Iteration {ev['iteration']}: Theme covered={ev['theme_covered']}, Gaps: {ev['gaps_identified']}"
        for ev in state.evaluation_history
    ]) if state.evaluation_history else "No evaluations yet - initial exploration"
    
    parts = [
        state.prior_coverage_context,
        f"Theme being analyzed:\n{state.theme}",
        f"Evaluation history:\n{eval_summary}",
        f"Available sections:\n{sections_json}",
        f"Explored sections: {state.explored_sections}"
    ]
    prompt = "\n\n".join(p for p in parts if p)
    
    result = await chat(prompt, response_format=SelectSectionOutput)
    parsed = parse_response(result)
    
    section_key = state.available_sections[parsed['section_index'] - 1]
    return {'section_key': section_key, 'reasoning': parsed['reasoning']}

# %% ../nbs/07_mappr.ipynb 58
async def summarize_content(
    state: State, # The current state of the analysis
    section_key: str, # The key of the section to summarize
    content: str, # The content of the section to summarize
    model: str = 'gemini/gemini-2.0-flash' # The model to use
) -> dict:
    "Summarize section content relevant to the theme"
    chat = AsyncChat(model=model, sp=summarize_sp, temp=0)
    
    parts = [
        state.prior_coverage_context,
        f"Theme: {state.theme}",
        f"Section: {section_key}",
        f"Content to summarize:\n{content}"
    ]
    prompt = "\n\n".join(p for p in parts if p)
    
    result = await chat(prompt, response_format=SummarizeContentOutput)
    return parse_response(result)

# %% ../nbs/07_mappr.ipynb 63
async def evaluate_evidence(
    state: State,
    new_content: str,
    model: str = 'gemini/gemini-2.0-flash'
) -> dict:
    "Evaluate evidence collected and determine if more exploration needed"
    chat = AsyncChat(model=model, sp=evaluate_evidence_sp, temp=0)
    
    # Format previous summaries
    prev_summaries = "\n\n".join([
        f"Section: {state.explored_sections[i]}\n"
        f"Summary: {state.section_summaries[i]['summary']}\n"
        f"Key findings: {', '.join(state.section_summaries[i]['key_findings'])}"
        for i in range(len(state.section_summaries))
    ]) if state.section_summaries else "None yet"
    
    # Format evaluation history
    prev_evaluations = "\n\n".join([
        f"Iteration {ev['iteration']}:\n"
        f"Theme covered: {ev['theme_covered']}\n"
        f"Reasoning: {ev['coverage_reasoning']}\n"
        f"Gaps: {ev['gaps_identified']}"
        for ev in state.evaluation_history
    ]) if state.evaluation_history else "First evaluation - no previous assessments"
    
    parts = [
        state.prior_coverage_context,
        f"Theme being analyzed:\n{state.theme}",
        f"Previous evidence summaries:\n{prev_summaries}",
        f"Previous evaluation reasoning:\n{prev_evaluations}",
        f"Exploration progress: {len(state.explored_sections)} sections explored out of {len(state.explored_sections) + len(state.available_sections)} total available",
        f"New content to evaluate:\n{new_content}"
    ]
    prompt = "\n\n".join(p for p in parts if p)
    
    result = await chat(prompt, response_format=EvaluateEvidenceOutput)
    return parse_response(result)

# %% ../nbs/07_mappr.ipynb 67
async def limit(semaphore, coro, delay=None):
    "Execute coroutine with semaphore concurrency control"
    async with semaphore:
        result = await coro
        if delay: await sleep(delay)
        return result

# %% ../nbs/07_mappr.ipynb 75
class Stage(Enum):
    "Pipeline stage number."
    STAGE1 = "stage1"
    STAGE2 = "stage2"
    STAGE3 = "stage3"
    def __str__(self): return self.value

# %% ../nbs/07_mappr.ipynb 76
class TraceContext(AttrDict):
    "Context for tracing the mapping process"
    def __init__(self, 
                 report_id:str,  # Report identifier
                 stage:Stage,  # Pipeline stage number
                 framework:FrameworkInfo,  # Framework info (name, category, theme_id)
                 ): 
        # self.run_id = str(uuid.uuid4())[:8]  # Short unique ID
        store_attr()

    def __repr__(self):
        return f"TraceContext(report_id={self.report_id}, stage={self.stage}, framework={self.framework})"


# %% ../nbs/07_mappr.ipynb 78
def setup_logger(name, handler, level=logging.INFO, **kwargs):
    "Helper function to setup a logger with common configuration"
    logger = logging.getLogger(name)
    logger.handlers.clear()
    logger.addHandler(handler)
    logger.setLevel(level)
    for k,v in kwargs.items(): setattr(logger, k, v)
    return logger

# %% ../nbs/07_mappr.ipynb 79
def setup_trace_logging(report_id, verbosity=cfg.verbosity):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'{report_id}_{timestamp}.jsonl'
    file_handler = logging.FileHandler(traces_dir / filename, mode='w')
    setup_logger('trace.file', file_handler)    
    console_handler = logging.StreamHandler()
    setup_logger('trace.console', console_handler, verbosity=verbosity)

# %% ../nbs/07_mappr.ipynb 80
def log_analysis_event(event: str, trace_ctx: TraceContext, **extra_data):
    """Log an analysis event to file and console with different verbosity levels"""
    file_logger = logging.getLogger('trace.file')
    console_logger = logging.getLogger('trace.console')
    
    base_data = {
        "timestamp": datetime.now().isoformat(),
        "event": event,
        "report_id": trace_ctx.report_id,
        "stage": str(trace_ctx.stage),
        "framework": str(trace_ctx.framework.name),
        "framework_category": str(trace_ctx.framework.category),
        "framework_theme_id": str(trace_ctx.framework.theme_id),
    }
    base_data.update(extra_data)
    
    # File logger - always full JSON
    file_logger.info(json.dumps(base_data, indent=2))
    
    # Console logger - verbosity-based formatting
    if hasattr(console_logger, 'verbosity'):
        if console_logger.verbosity == 1:
            console_msg = f"{base_data['report_id']} - {base_data['stage']}"
        elif console_logger.verbosity == 2:
            console_msg = f"{base_data['report_id']} - {base_data['stage']} - {base_data['event']}"
        else:  # verbosity == 3
            console_msg = json.dumps(base_data, indent=2)
        
        console_logger.info(console_msg)

# %% ../nbs/07_mappr.ipynb 90
class SectionSelection(dspy.Signature):
    "Choose the next most relevant section based on current evidence summary and gaps."
    theme: str = dspy.InputField(desc="Theme being analyzed")
    evidence_summary: str = dspy.InputField(desc="Current summary of key evidence", default="No evidence collected yet - beginning analysis")
    gaps_identified: str = dspy.InputField(desc="Knowledge gaps to address", default="No gaps identified yet - initial exploration")
    all_headings: str = dspy.InputField(desc="Complete document structure")
    sections_explored: str = dspy.InputField(desc="Sections already explored", default="")
    next_section: str = dspy.OutputField(desc="Next section key to explore - must be an exact key from all_headings and NOT in sections_explored, or 'DONE'")
    reasoning: str = dspy.OutputField(desc="Why this section was chosen")

# %% ../nbs/07_mappr.ipynb 93
class Assessment(dspy.Signature):
    "Assess evidence sufficiency and update running summary by incorporating new evidence. Calculate confidence as coverage completeness percentage."
    theme: str = dspy.InputField(desc="Theme being analyzed with key aspects to cover")
    evidence_summary: str = dspy.InputField(desc="Current summary of key evidence", default="No evidence collected yet - beginning analysis")
    gaps_identified: str = dspy.InputField(desc="Knowledge gaps from previous assessment", default="No gaps identified yet - initial exploration")
    new_evidence: str = dspy.InputField(desc="New evidence just collected from the latest section")
    sections_explored: str = dspy.InputField(desc="Sections already checked", default="")
    sufficient: bool = dspy.OutputField(desc="Is evidence sufficient?")
    confidence_score: float = dspy.OutputField(desc="Coverage completeness: 0.0-1.0 representing what percentage of theme's key aspects have been addressed")
    updated_evidence_summary: str = dspy.OutputField(desc="Updated summary incorporating the new evidence")
    updated_gaps: str = dspy.OutputField(desc="Updated knowledge gaps after reviewing new evidence")
    reasoning: str = dspy.OutputField(desc="Assessment reasoning including which key aspects are covered/missing")


# %% ../nbs/07_mappr.ipynb 95
class Stage(Enum):
    "Pipeline stage number."
    STAGE1 = "stage1"
    STAGE2 = "stage2"
    STAGE3 = "stage3"
    def __str__(self): return self.value
#| exports
class TraceContext(AttrDict):
    "Context for tracing the mapping process"
    def __init__(self, 
                 report_id:str,  # Report identifier
                 stage:Stage,  # Pipeline stage number
                 framework:FrameworkInfo,  # Framework info (name, category, theme_id)
                 ): 
        # self.run_id = str(uuid.uuid4())[:8]  # Short unique ID
        store_attr()

    def __repr__(self):
        return f"TraceContext(report_id={self.report_id}, stage={self.stage}, framework={self.framework})"
#| eval: false
tr_ctx = TraceContext(
    report_id='49d2fba781b6a7c0d94577479636ee6f', 
    stage=Stage.STAGE1, 
    framework=FrameworkInfo(Framework.SRF, FrameworkCat.ENABLERS, "4")
    )

tr_ctx

# %% ../nbs/07_mappr.ipynb 96
class Synthesis(dspy.Signature):
    "Provide detailed rationale and synthesis of theme analysis."
    trace_ctx: str = dspy.InputField(desc="Trace context")
    theme: str = dspy.InputField(desc="Theme being analyzed")
    evidence_summary: str = dspy.InputField(desc="Final summary of key evidence")
    gaps_identified: str = dspy.InputField(desc="Final knowledge gaps")
    sections_explored: str = dspy.InputField(desc="List of sections explored")
    theme_covered: bool = dspy.OutputField(desc="Final decision on theme coverage")
    confidence_explanation: str = dspy.OutputField(desc="Detailed explanation of confidence score")
    evidence_summary: str = dspy.OutputField(desc="Key evidence supporting the conclusion")
    gaps_identified: str = dspy.OutputField(desc="Any gaps or missing aspects")

# %% ../nbs/07_mappr.ipynb 99
def setup_logger(name, handler, level=logging.INFO, **kwargs):
    "Helper function to setup a logger with common configuration"
    logger = logging.getLogger(name)
    logger.handlers.clear()
    logger.addHandler(handler)
    logger.setLevel(level)
    for k,v in kwargs.items(): setattr(logger, k, v)
    return logger

# %% ../nbs/07_mappr.ipynb 100
def setup_trace_logging(report_id, verbosity=cfg.verbosity):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'{report_id}_{timestamp}.jsonl'
    file_handler = logging.FileHandler(traces_dir / filename, mode='w')
    setup_logger('trace.file', file_handler)    
    console_handler = logging.StreamHandler()
    setup_logger('trace.console', console_handler, verbosity=verbosity)

# %% ../nbs/07_mappr.ipynb 101
class ThemeAnalyzer(dspy.Module):
    """
    Analyzes a theme across a document by iteratively exploring sections, collecting evidence, and synthesizing findings. 
    Uses a structured pipeline of section selection -> assessment -> synthesis.
    """
    def __init__(self, 
                 section_selection_sig: dspy.Signature,
                 assessment_sig: dspy.Signature, 
                 synthesis_sig: dspy.Signature, 
                 trace_ctx: TraceContext,
                 confidence_threshold: float = 0.8,
                 max_iter: int = cfg.max_iter,
                 semaphore = None):
        self.section_selector = dspy.ChainOfThought(section_selection_sig)
        self.assess = dspy.ChainOfThought(assessment_sig)
        self.synthesize = dspy.ChainOfThought(synthesis_sig)
        self.max_iter = max_iter
        self.trace_ctx = trace_ctx
        self.confidence_threshold = confidence_threshold
        self.semaphore = semaphore


# %% ../nbs/07_mappr.ipynb 102
@patch
async def aforward(
    self:ThemeAnalyzer, 
    theme: str, # The formatted theme to analyze
    hdgs: dict, # The headings TOC of the document
    get_content_fn: Callable = get_content_tool, # The function to get the content of a section using `hdgs[keys_list].text` for instance
    prior_coverage_context: str = "" # The themes already covered in this report, indicating its scope and analytical focus
) -> Synthesis:
    "Executes a structured analysis process."
    self._log_trace(event="Starting Analysis", theme=theme)
    
    # Main iterative exploration
    evidence = await self.explore_iteratively(theme, hdgs, get_content_fn, prior_coverage_context)
    
    # Final synthesis with summary and gaps from last assessment
    return await self.synthesize_findings(
        theme, 
        evidence["final_summary"], 
        evidence["final_gaps"], 
        evidence["sections"], 
        prior_coverage_context
    )

# %% ../nbs/07_mappr.ipynb 103
@patch
async def explore_iteratively(
    self:ThemeAnalyzer, 
    theme: str,
    hdgs: dict,
    get_content_fn: Callable,
    prior_coverage_context: str = ""
) -> dict:
    "Iteratively explore sections to collect evidence."
    evidence_collected = []
    sections_explored = []
    evidence_summary = "No evidence collected yet - beginning analysis"
    gaps = "No gaps identified yet - initial exploration"
    
    for i in range(self.max_iter):
        # 1. Select next section
        decision = await self.select_next_section(
            theme, evidence_summary, gaps, str(hdgs), sections_explored, prior_coverage_context)
        
        if decision.next_section == 'DONE':
            self._log_trace(event="Iterative Exploration", iteration_nb=i+1, decision="Done")
            break
            
        # # 2. Process section
        # evidence_collected, sections_explored = self.process_section(
        #     decision, hdgs, get_content_fn, evidence_collected, sections_explored, [])
        
        # # 3. Assess and update summary/gaps
        # assessment = await self.assess_evidence(
        #     theme, evidence_summary, gaps, sections_explored, prior_coverage_context)
        # 2. Process section
        old_evidence_count = len(evidence_collected)
        evidence_collected, sections_explored = self.process_section(decision, hdgs, get_content_fn, evidence_collected, sections_explored, [])

        # Extract new evidence
        new_evidence = evidence_collected[old_evidence_count:] if len(evidence_collected) > old_evidence_count else ""
        new_evidence_text = "\n".join(new_evidence) if new_evidence else "No new evidence found"

        # 3. Assess and update summary/gaps  
        assessment = await self.assess_evidence(
            theme, evidence_summary, gaps, new_evidence_text, sections_explored, prior_coverage_context)
        evidence_summary = assessment.updated_evidence_summary
        gaps = assessment.updated_gaps
        
        if assessment.sufficient and assessment.confidence_score > self.confidence_threshold:
            break
    
    return {
        "evidence": evidence_collected,
        "sections": sections_explored,
        "final_summary": evidence_summary,
        "final_gaps": gaps
    }


# %% ../nbs/07_mappr.ipynb 104
@patch
async def assess_evidence(
    self:ThemeAnalyzer, 
    theme: str,
    evidence_summary: str,
    gaps: str,
    new_evidence: str,
    sections_explored: list,
    prior_coverage_context: str = ""
):
    assessment = await self._rate_limited_fn(
        self.assess,
        theme=theme,
        evidence_summary=evidence_summary,
        gaps_identified=gaps,
        new_evidence=new_evidence,
        sections_explored=str(sections_explored),
        prior_coverage_context=prior_coverage_context
    )
    
    # Log the assessment
    self._log_trace(
        event="Evidence Assessment",
        sufficient=assessment.sufficient,
        confidence=assessment.confidence_score,
        updated_evidence_summary=assessment.updated_evidence_summary,
        updated_gaps=assessment.updated_gaps,
        sections_explored=sections_explored, 
        reasoning=assessment.reasoning
    )
    
    return assessment


# %% ../nbs/07_mappr.ipynb 105
@patch
def process_section(
    self:ThemeAnalyzer, 
    decision:SectionSelection, # The next section to explore
    hdgs: dict, # The headings TOC of the document
    get_content_fn: Callable, # The function to get the content of a section using `hdgs[keys_list].text` for instance
    evidence_collected: list, # The evidence collected so far
    sections_explored: list, # The sections explored so far
    available_sections: list # Not used anymore but kept for compatibility
):
    evidence_collected = evidence_collected.copy()
    sections_explored = sections_explored.copy()
    
    path = find_section_path(hdgs, decision.next_section)
    if path:
        content = get_content_fn(hdgs, path)
        evidence_collected.append(f"# Section: {decision.next_section}\n## Content\n{content}")
        sections_explored.append(decision.next_section)
        self._log_trace(
            event="Section Found", 
            section=decision.next_section
        )
    else:
        self._log_trace(
            event="Section Not Found", 
            section=decision.next_section, 
            warning="No path found for section"
        )
    
    return evidence_collected, sections_explored

# %% ../nbs/07_mappr.ipynb 106
@patch
async def select_next_section(
    self:ThemeAnalyzer, 
    theme: str, # The formatted theme to analyze
    evidence_summary: str, # The summary of the evidence collected so far
    gaps: str, # The gaps identified so far
    hdgs: dict, # The headings TOC of the document
    sections_explored: list, # The sections explored so far
    prior_coverage_context: str = "" # The themes already covered in this report, indicating its scope and analytical focus
):
    decision = await self._rate_limited_fn(
        self.section_selector,
        theme=theme,
        evidence_summary=evidence_summary,
        gaps_identified=gaps,
        all_headings=str(hdgs),
        sections_explored=str(sections_explored),
        prior_coverage_context=prior_coverage_context
    )
    
    # Log the section selection
    self._log_trace(
        event="Section Selection",
        selected_section=decision.next_section,
        reasoning=decision.reasoning
    )
    
    return decision

# %% ../nbs/07_mappr.ipynb 107
@patch
async def synthesize_findings(
    self:ThemeAnalyzer, 
    theme: str,
    evidence_summary: str,
    gaps: str,
    sections_explored: list,
    prior_coverage_context: str = ""
):
    synthesis = await self._rate_limited_fn(
        self.synthesize,
        trace_ctx=str(self.trace_ctx),
        theme=theme,
        evidence_summary=evidence_summary,
        gaps_identified=gaps,
        sections_explored=str(sections_explored),
        prior_coverage_context=prior_coverage_context
    )
    
    # Log synthesis results
    self._log_trace(
        event="Synthesis",
        theme_covered=synthesis.theme_covered,
        confidence_explanation=synthesis.confidence_explanation,
        evidence_summary=synthesis.evidence_summary,
        gaps_identified=synthesis.gaps_identified
    )
    
    # Add framework metadata
    synthesis.framework_name = self.trace_ctx.framework.name
    synthesis.framework_category = self.trace_ctx.framework.category  
    synthesis.framework_theme_id = self.trace_ctx.framework.theme_id
    return synthesis


# %% ../nbs/07_mappr.ipynb 108
@patch
def _log_trace(self:ThemeAnalyzer, event, **extra_data):
    file_logger = logging.getLogger('trace.file')
    console_logger = logging.getLogger('trace.console')
    
    base_data = {
        "timestamp": datetime.now().isoformat(),
        "event": event,
        "report_id": self.trace_ctx.report_id,
        "stage": str(self.trace_ctx.stage),
        "framework": str(self.trace_ctx.framework.name),
        "framework_category": str(self.trace_ctx.framework.category),
        "framework_theme_id": str(self.trace_ctx.framework.theme_id),
    }
    base_data.update(extra_data)
    
    # File logger - always full JSON
    file_logger.info(json.dumps(base_data, indent=2))
    
    # Console logger - verbosity-based formatting
    if hasattr(console_logger, 'verbosity'):
        if console_logger.verbosity == 1:
            console_msg = f"{base_data['report_id']} - {base_data['stage']}"
        elif console_logger.verbosity == 2:
            console_msg = f"{base_data['report_id']} - {base_data['stage']} - {base_data['framework']} - {base_data['framework_category']} - {base_data['framework_theme_id']} - {base_data['event']}"
        else:  # verbosity == 3
            console_msg = json.dumps(base_data, indent=2)
        
        console_logger.info(console_msg)

# %% ../nbs/07_mappr.ipynb 109
@patch    
async def _rate_limited_fn(self:ThemeAnalyzer, mod, **kwargs):
    async with self.semaphore:
        start = time.time()
        result = await mod.acall(**kwargs)
        
        # Check if cached (fast response + no usage)
        elapsed = time.time() - start
        if elapsed > cfg.cache.delay: await sleep(cfg.call_delay)
        return result

# %% ../nbs/07_mappr.ipynb 131
class PipelineResults(dict):
    def __init__(self):
        super().__init__()
        self[Stage.STAGE1] = defaultdict(lambda: defaultdict(dict))
        self[Stage.STAGE2] = defaultdict(lambda: defaultdict(dict))
        self[Stage.STAGE3] = defaultdict(lambda: defaultdict(dict))

# %% ../nbs/07_mappr.ipynb 132
@patch
def __call__(self:PipelineResults, stage=Stage.STAGE1, filter_type="all"):
    themes = []
    for frameworks in self[stage].values():
        for categories in frameworks.values():
            for theme in categories.values():
                if filter_type == "all" or \
                   (filter_type == "covered" and theme.theme_covered) or \
                   (filter_type == "uncovered" and not theme.theme_covered):
                    themes.append(theme)
    return themes

# %% ../nbs/07_mappr.ipynb 133
class PipelineOrchestrator:
    "Orchestrator for the IOM evaluation report mapping pipeline"
    def __init__(self, 
                 report_id:str, # Report identifier
                 headings:dict, # Report headings
                 get_content_fn:Callable, # Function to get the content of a section
                 eval_data:EvalData, # Evaluation data
                 verbosity:int=2, # Verbosity level
                 ):
        store_attr()
        setup_trace_logging(report_id, verbosity)
        self.results = PipelineResults()

# %% ../nbs/07_mappr.ipynb 134
@patch
async def run_stage1(self:PipelineOrchestrator, semaphore):
    "Run stage 1 of the pipeline"
    setup_trace_logging(self.report_id, self.verbosity)
    analyzers = []
    
    collections = [
        (self.eval_data.srf_enablers, FrameworkCat.ENABLERS, format_enabler_theme),
        (self.eval_data.srf_crosscutting_priorities, FrameworkCat.CROSSCUT, format_crosscutting_theme)
    ]

    for items, framework_cat, format_fn in collections:
        for item in items:
            trace_ctx = TraceContext(self.report_id, Phase.STAGE1, FrameworkInfo(Framework.SRF, framework_cat, item.id))
            theme = format_fn(item)
            analyzer = ThemeAnalyzer(Overview, Exploration, Assessment, Synthesis, trace_ctx, semaphore=semaphore)
            analyzers.append((analyzer, theme))

    results = await gather(*[analyzer.acall(theme, self.headings, self.get_content_fn) 
                             for analyzer, theme in analyzers])
    for result in results: 
        self.results[Phase.STAGE1][result.framework_name][result.framework_category][result.framework_theme_id] = result

# %% ../nbs/07_mappr.ipynb 138
def get_stage1_covered_context(results: PipelineResults, eval_data: EvalData) -> str:
    "Get and format covered themes in Stage 1."
    covered_themes = results(Phase.STAGE1, filter_type="covered")
    if not covered_themes: return ""
    
    context_parts = []
    for theme in covered_themes:
        if theme.framework_category == str(FrameworkCat.ENABLERS):
            theme_data = next(t for t in eval_data.srf_enablers if t.id == theme.framework_theme_id)
        elif theme.framework_category == str(FrameworkCat.CROSSCUT):
            theme_data = next(t for t in eval_data.srf_crosscutting_priorities if t.id == theme.framework_theme_id)
        
        context_parts.append(f"- **{theme.framework_category} {theme_data.id}**: {theme_data.title}")
    
    return f"### Report Preliminary Context\nThis evaluation report covers the following Strategic Results Framework themes:\n" + "\n".join(context_parts)


# %% ../nbs/07_mappr.ipynb 141
@patch
async def run_stage2(self:PipelineOrchestrator, semaphore):
    "Run stage 2 of the pipeline - GCM objectives analysis"
    setup_trace_logging(self.report_id, self.verbosity)
    stage1_context = get_stage1_covered_context(self.results, self.eval_data)
    analyzers = []
    
    for gcm_obj in gcm_small:
        trace_ctx = TraceContext(self.report_id, Phase.STAGE2, FrameworkInfo(Framework.GCM, FrameworkCat.OBJS, gcm_obj["id"]))
        theme = format_gcm_theme(gcm_obj)
        analyzer = ThemeAnalyzer(Overview, Exploration, Assessment, Synthesis, trace_ctx, semaphore=semaphore)
        analyzers.append((analyzer, theme, stage1_context))

    results = await gather(*[analyzer.acall(theme, self.headings, self.get_content_fn, context) 
                             for analyzer, theme, context in analyzers])
    
    for result in results: 
        self.results[Phase.STAGE2][result.framework_name][result.framework_category][result.framework_theme_id] = result

# %% ../nbs/07_mappr.ipynb 144
def get_filtered_srf_output_ids(
    results: PipelineResults, # PipelineResults
    eval_data: EvalData # EvalData
    ) -> list: # list of SRF output IDs
    "Get filtered SRF output IDs based on covered GCM themes."
    covered_gcm = results(Phase.STAGE2, filter_type="covered")
    srf_output_ids = set()
    
    for gcm_theme in covered_gcm:
        gcm_id = gcm_theme.framework_theme_id
        if gcm_id in eval_data.gcm_srf_lut:
            srf_output_ids.update(eval_data.gcm_srf_lut[gcm_id])
    
    return list(srf_output_ids)

# %% ../nbs/07_mappr.ipynb 147
def get_combined_context(
    results: PipelineResults, # PipelineResults
    eval_data: EvalData, # EvalData
    ) -> str: # combined context
    "Get combined context from previous stages (1 and 2)."
    stage1_context = get_stage1_covered_context(results, eval_data)
    covered_gcm = results(Phase.STAGE2, filter_type="covered")
    
    if not covered_gcm: return stage1_context
    
    gcm_context = "\n".join([f"- **GCM {theme.framework_theme_id}**: {eval_data.gcm_objectives_small[int(theme.framework_theme_id)-1]['title']}" 
                            for theme in covered_gcm])
    
    return f"{stage1_context}\n\n### Covered GCM Objectives\n{gcm_context}"


# %% ../nbs/07_mappr.ipynb 150
@patch
async def run_stage3(self:PipelineOrchestrator, semaphore):
    "Run stage 3 of the pipeline - Targeted SRF outputs analysis"
    setup_trace_logging(self.report_id, self.verbosity)
    
    combined_context = get_combined_context(self.results, self.eval_data)
    filtered_output_ids = get_filtered_srf_output_ids(self.results, self.eval_data)
    analyzers = []
    
    for output_id in filtered_output_ids:
        output_context = find_srf_output_by_id(self.eval_data, output_id)
        if output_context:
            trace_ctx = TraceContext(self.report_id, Phase.STAGE3, FrameworkInfo(Framework.SRF, FrameworkCat.OUTPUTS, output_id))
            theme = format_srf_output(output_context)
            analyzer = ThemeAnalyzer(Overview, Exploration, Assessment, Synthesis, trace_ctx, semaphore=semaphore)
            analyzers.append((analyzer, theme, combined_context))

    results = await gather(*[analyzer.acall(theme, self.headings, self.get_content_fn, context) 
                             for analyzer, theme, context in analyzers])
    
    for result in results: 
        self.results[Phase.STAGE3][result.framework_name][result.framework_category][result.framework_theme_id] = result
